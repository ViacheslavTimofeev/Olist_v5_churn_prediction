from __future__ import annotations
import json, glob, os, yaml, typer
from pathlib import Path
from typing import Dict, Any, Union, List, Callable
import pandas as pd

from olist_churn_prediction import feature_processing as fp  # lowercase_categoricals, disambiguate_city_state, group_by_features

app = typer.Typer(help="Preprocessing pipeline CLI (manifest-driven)")

''' –ó–∞–≥—Ä—É–∑–∫–∞/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ‚Äî –ø–æ –º–æ—Ç–∏–≤–∞–º validator_cli.py '''
def _load_df(entry: Dict[str, Any]) -> pd.DataFrame:
    """
    –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–∞–∫ –≤ validator_cli.py:
      - file: input —Å glob-–º–∞—Å–∫–æ–π (–±–µ—Ä—ë–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
      - sql:  query + conn_env (—Å—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    """
    reader = entry.get("reader", "file")
    if reader == "sql":
        import sqlalchemy as sa
        url = os.environ[entry["conn_env"]]
        engine = sa.create_engine(url)
        return pd.read_sql(sa.text(entry["query"]), engine, params=entry.get("params"))
    else:
        path = sorted(glob.glob(entry["input"]))[-1]
        return pd.read_parquet(path) if path.endswith(".parquet") else pd.read_csv(path)

def _save_df(df: pd.DataFrame, output: str):
    out = Path(output)
    out.parent.mkdir(parents=True, exist_ok=True)
    if out.suffix == ".parquet":
        df.to_parquet(out, index=False)
    else:
        df.to_csv(out, index=False)
    typer.echo(f"üíæ Saved: {out}")

''' –Ø–¥—Ä–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–≥–æ–≤ '''
def _apply_steps(
    df: pd.DataFrame,
    steps: List[Dict[str, Any]],
    defaults: Dict[str, Any] | None = None,
) -> pd.DataFrame:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ –∫ DataFrame.
    –ö–∞–∂–¥—ã–π —à–∞–≥ ‚Äî dict —Å –∫–ª—é—á–æ–º 'op' –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –ü—Ä–∏–º–µ—Ä—ã —Å–º. –Ω–∏–∂–µ.
    """
    defaults = defaults or {}
    X = df

    for i, step in enumerate(steps, start=1):
        op = step.get("op")
        if not op:
            raise ValueError(f"Step #{i} has no 'op'")

        if op == "lowercase_categoricals":
            cat_cols = step.get("cat_cols")
            if cat_cols is None:
                # –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ object/category (–±–µ–∑ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –º–æ–¥—É–ª–µ)
                cat_cols = [c for c in X.columns if str(X[c].dtype) in ("object", "string") or pd.api.types.is_categorical_dtype(X[c])]
            X = fp.lowercase_categoricals(X, cat_cols=cat_cols, inplace=False)

        elif op == "disambiguate_city_state":
            city_col = step["city_col"]
            state_col = step["state_col"]
            suffix_sep = step.get("suffix_sep", "_")
            X = fp.disambiguate_city_state(X, city_col, state_col, suffix_sep=suffix_sep, inplace=False)

        elif op == "group_by_features":
            group_mapping = step["group_mapping"]             # {"new_feat": ["col1","col2"], ...}
            agg_funcs     = step.get("agg_funcs", "sum")      # –º–æ–∂–µ—Ç –±—ã—Ç—å str|callable|list|dict
            keep_original = step.get("keep_original", False)
            prefix        = step.get("prefix")
            X = fp.group_by_features(
                X, group_mapping=group_mapping, agg_funcs=agg_funcs,
                keep_original=keep_original, prefix=prefix
            )

        elif op == "groupby_aggregate":
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É(–∞–º) —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–≥–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞–º–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º.
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            #   by: str | list[str]        ‚Äî –∫–ª—é—á(–∏) –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            #   sum_cols:  list[str]
            #   mean_cols: list[str]
            #   min_cols:  list[str]
            #   first_for_rest: bool=True  ‚Äî –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –±–µ—Ä—ë–º 'first'
            by = step["by"]
            if isinstance(by, str):
                by = [by]

            sum_cols  = step.get("sum_cols", []) or []
            mean_cols = step.get("mean_cols", []) or []
            min_cols  = step.get("min_cols", []) or []
            first_for_rest = bool(step.get("first_for_rest", True))

            # 1) —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –∞–≥–≥—Ä–µ–≥–∞—Ü–∏–π
            agg_dict = {}
            for c in sum_cols:  agg_dict[c]  = "sum"
            for c in mean_cols: agg_dict[c] = "mean"
            for c in min_cols:  agg_dict[c]  = "min"

            # 2) –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Äî 'first' (–∫—Ä–æ–º–µ –∫–ª—é—á–µ–π –∏ —É–∂–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö)
            if first_for_rest:
                selected = set(by) | set(sum_cols) | set(mean_cols) | set(min_cols)
                for c in X.columns:
                    if c not in selected:
                        agg_dict[c] = "first"

            # 3) —Å–∞–º groupby
            # dropna=False —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –≥—Ä—É–ø–ø—ã —Å NaN-–∫–ª—é—á–æ–º (–Ω–∞ —Ç–≤–æ–π –≤–∫—É—Å –º–æ–∂–Ω–æ True).
            X = (
                X.groupby(by, dropna=False)
                 .agg(agg_dict)
                 .reset_index()
            )

        elif op == "rename_columns":
            # —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–π —à–∞–≥: {"old":"new", ...}
            mapping = step["mapping"]
            X = X.rename(columns=mapping)

        elif op == "drop_columns":
            cols = step["cols"]
            X = X.drop(columns=cols, errors="ignore")

        else:
            raise ValueError(f"Unknown op '{op}' in step #{i}")

    return X

# ======== –ö–æ–º–∞–Ω–¥—ã CLI ========

@app.command()
def apply(
    input: str = typer.Argument(..., help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.csv/.parquet)"),
    output: str = typer.Argument(..., help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"),
    steps_json: str = typer.Option(None, help="JSON —Å–æ —Å–ø–∏—Å–∫–æ–º —à–∞–≥–æ–≤"),
    sample: float = typer.Option(None, help="–î–æ–ª—è —Å—ç–º–ø–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –Ω–∞–ø—Ä. 0.1"),
):
    """
    –ü—Ä–∏–º–µ–Ω–∏—Ç—å —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ –æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (–±–µ–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞).
    –ü—Ä–∏–º–µ—Ä:
      preproc apply data/raw.csv data/interim/cli_related/clean.parquet \\
        --steps-json '[{"op":"lowercase_categoricals", "cat_cols":["customer_city", "customer_state"]}]'
    """
    df = pd.read_parquet(input) if input.endswith(".parquet") else pd.read_csv(input)
    if sample:
        df = df.sample(frac=float(sample), random_state=42)
    steps = json.loads(steps_json) if steps_json else []
    df_out = _apply_steps(df, steps)
    _save_df(df_out, output)

@app.command()
def run(manifest: str = typer.Option("preprocessings/preprocessing_manifest.yaml", "--manifest", "-m", help="–ü—É—Ç—å –∫ YAML-–º–∞–Ω–∏—Ñ–µ—Å—Ç—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –Ω–∞–±–æ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ –º–∞–Ω–∏—Ñ–µ—Å—Ç—É YAML.
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø–æ –¥—É—Ö—É —Å validator_cli: defaults + datasets[].
    """
    cfg = yaml.safe_load(Path(manifest).read_text())
    defaults = cfg.get("defaults", {})

    errors_total: list[str] = []

    for ds in cfg["datasets"]:
        if ds.get("enabled") is False:
            continue

        name = ds["name"]
        typer.echo(f"‚ñ∂ {name}")

        # 1) –∑–∞–≥—Ä—É–∑–∫–∞
        try:
            df = _load_df(ds)   # —Ç–∞ –∂–µ –∏–¥–µ—è, —á—Ç–æ –∏ –≤ validator_cli
        except Exception as e:
            msg = f"load error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 2) sample (–∫–∞–∫ –≤ validator_cli.profile_all)
        sample = ds.get("sample", defaults.get("sample"))
        if sample:
            df = df.sample(frac=float(sample), random_state=42)

        # 3) —à–∞–≥–∏
        try:
            steps = ds.get("steps", [])
            df_out = _apply_steps(df, steps, defaults=defaults)
        except Exception as e:
            msg = f"processing error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 4) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        try:
            output = ds["output"]
            _save_df(df_out, output)
            typer.echo(f"‚úÖ {name}: OK")
        except Exception as e:
            msg = f"save error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")

    if errors_total:
        raise typer.Exit(code=1)

    typer.echo("‚úÖ –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")

if __name__ == "__main__":
    app()
