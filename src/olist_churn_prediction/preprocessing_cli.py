"""preprocessing_cli ‚Äî –º–∞–Ω–∏—Ñ–µ—Å—Ç-driven –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞.

–°–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫/—Å–æ—Ö—Ä–∞–Ω—è–ª–∫—É –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏—é `_apply_steps()`,
–∫–æ—Ç–æ—Ä—É—é –∏—Å–ø–æ–ª—å–∑—É—é—Ç CLI-–∫–æ–º–∞–Ω–¥—ã `apply` –∏ `run`.
"""
from __future__ import annotations
import json, glob, os, yaml, typer
from typer.main import get_command
from pathlib import Path
from typing import Dict, Any, Union, List, Callable
import pandas as pd
from olist_churn_prediction.targets import create_churn_label
from olist_churn_prediction import feature_processing as fp  # lowercase_categoricals, disambiguate_city_state, group_by_features

app = typer.Typer(help="Preprocessing pipeline CLI (manifest-driven)")

''' –ó–∞–≥—Ä—É–∑–∫–∞/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ‚Äî –ø–æ –º–æ—Ç–∏–≤–∞–º validator_cli.py '''
def _load_df(entry: Dict[str, Any]) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DataFrame –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å ``validator_cli.py``).

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–µ–∂–∏–º—ã:
        - ``"file"``: —á–∏—Ç–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Ñ–∞–π–ª –ø–æ glob-–º–∞—Å–∫–µ –∏–∑ ``entry["input"]``.
        - ``"sql"``: –≤—ã–ø–æ–ª–Ω—è–µ—Ç SQL-–∑–∞–ø—Ä–æ—Å ``entry["query"]`` —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
          –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è ``entry["conn_env"]``.

    Args:
        entry (Dict[str, Any]): –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –ö–ª—é—á–∏:
            - ``reader`` (str, optional): ``"file"`` –∏–ª–∏ ``"sql"``. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ``"file"``.
            - –¥–ª—è ``"file"``: ``input`` (str) ‚Äî glob-–º–∞—Å–∫–∞ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º.
            - –¥–ª—è ``"sql"``: ``query`` (str) –∏ ``conn_env`` (str) ‚Äî –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
              —Å–æ —Å—Ç—Ä–æ–∫–æ–π –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è.

    Returns:
        pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

    Raises:
        KeyError: –ï—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞.
        FileNotFoundError: –ï—Å–ª–∏ –ø–æ glob-–º–∞—Å–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        ValueError: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ``reader``.
    """
    reader = entry.get("reader", "file")
    if reader == "sql":
        import sqlalchemy as sa
        url = os.environ[entry["conn_env"]]
        engine = sa.create_engine(url)
        return pd.read_sql(sa.text(entry["query"]), engine, params=entry.get("params"))
    else:
        path = sorted(glob.glob(entry["input"]))[-1]
        return pd.read_parquet(path) if path.endswith(".parquet") else pd.read_csv(path)

def _save_df(df: pd.DataFrame, output: str):
    out = Path(output)
    out.parent.mkdir(parents=True, exist_ok=True)
    if out.suffix == ".parquet":
        df.to_parquet(out, index=False)
    else:
        df.to_csv(out, index=False)
    typer.echo(f"üíæ Saved: {out}")

''' –Ø–¥—Ä–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–≥–æ–≤ '''
def _apply_steps(
    df: pd.DataFrame,
    steps: List[Dict[str, Any]],
    defaults: Dict[str, Any] | None = None,
) -> pd.DataFrame:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ DataFrame.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —à–∞–≥–∏: `lowercase_categoricals`, `disambiguate_city_state`,
    `group_by_features`, `groupby_aggregate`, `dropna_rows`, `dropna_columns`,
    `drop_duplicates`, `rename_columns`, `drop_columns`, `select_columns`, `join`.

    Args:
        df: –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
        steps: –°–ø–∏—Å–æ–∫ –æ–ø–µ—Ä–∞—Ü–∏–π (–∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî dict —Å –∫–ª—é—á–æ–º `op` –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏).
        defaults: –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞.

    Returns:
        –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π DataFrame –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —à–∞–≥–æ–≤.

    Raises:
        ValueError: –ï—Å–ª–∏ —à–∞–≥ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á `op` –∏–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π `op`.
    """
    defaults = defaults or {}
    X = df

    for i, step in enumerate(steps, start=1):
        op = step.get("op")
        if not op:
            raise ValueError(f"Step #{i} has no 'op'")

        if op == "lowercase_categoricals":
            cat_cols = step.get("cat_cols")
            if cat_cols is None:
                # –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ object/category (–±–µ–∑ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ –º–æ–¥—É–ª–µ)
                cat_cols = [c for c in X.columns if str(X[c].dtype) in ("object", "string") or pd.api.types.is_categorical_dtype(X[c])]
            X = fp.lowercase_categoricals(X, cat_cols=cat_cols, inplace=False)

        elif op == "disambiguate_city_state":
            city_col = step["city_col"]
            state_col = step["state_col"]
            suffix_sep = step.get("suffix_sep", "_")
            X = fp.disambiguate_city_state(X, city_col, state_col, suffix_sep=suffix_sep, inplace=False)

        elif op == "group_by_features":
            group_mapping = step["group_mapping"]             # {"new_feat": ["col1","col2"], ...}
            agg_funcs     = step.get("agg_funcs", "sum")      # –º–æ–∂–µ—Ç –±—ã—Ç—å str|callable|list|dict
            keep_original = step.get("keep_original", False)
            prefix        = step.get("prefix")
            X = fp.group_by_features(
                X, group_mapping=group_mapping, agg_funcs=agg_funcs,
                keep_original=keep_original, prefix=prefix
            )

        elif op == "groupby_aggregate":
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É(–∞–º) —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–≥–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞–º–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º.
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            #   by: str | list[str]        ‚Äî –∫–ª—é—á(–∏) –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            #   sum_cols:  list[str]
            #   mean_cols: list[str]
            #   min_cols:  list[str]
            #   first_for_rest: bool=True  ‚Äî –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –±–µ—Ä—ë–º 'first'
            by = step["by"]
            if isinstance(by, str):
                by = [by]

            sum_cols  = step.get("sum_cols", []) or []
            mean_cols = step.get("mean_cols", []) or []
            min_cols  = step.get("min_cols", []) or []
            first_for_rest = bool(step.get("first_for_rest", True))

            # 1) —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –∞–≥–≥—Ä–µ–≥–∞—Ü–∏–π
            agg_dict = {}
            for c in sum_cols:  agg_dict[c]  = "sum"
            for c in mean_cols: agg_dict[c] = "mean"
            for c in min_cols:  agg_dict[c]  = "min"

            # 2) –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Äî 'first' (–∫—Ä–æ–º–µ –∫–ª—é—á–µ–π –∏ —É–∂–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö)
            if first_for_rest:
                selected = set(by) | set(sum_cols) | set(mean_cols) | set(min_cols)
                for c in X.columns:
                    if c not in selected:
                        agg_dict[c] = "first"

            # 3) —Å–∞–º groupby
            # dropna=False —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –≥—Ä—É–ø–ø—ã —Å NaN-–∫–ª—é—á–æ–º (–Ω–∞ —Ç–≤–æ–π –≤–∫—É—Å –º–æ–∂–Ω–æ True).
            X = (
                X.groupby(by, dropna=False)
                 .agg(agg_dict)
                 .reset_index()
            )
            
        elif op == "dropna_rows":
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏.
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        #   subset: str | list[str] ‚Äî –ø–æ –∫–∞–∫–∏–º –∫–æ–ª–æ–Ω–∫–∞–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å NaN (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî –ø–æ –≤—Å–µ–º)
        #   how: "any"|"all"        ‚Äî —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å –ª—é–±–æ–π NaN ("any") –∏–ª–∏ –≤—Å–µ NaN ("all")
        #   thresh: int             ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ù–ï-–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –æ—Å—Ç–∞–ª–∞—Å—å (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ, 'how' –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
            subset = step.get("subset")
            if isinstance(subset, str):
                subset = [subset]

            thresh = step.get("thresh", None)
            before = len(X)

            if thresh is not None:
                # –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ –ø—Ä–∞–≤–∏–ª—É "–æ—Å—Ç–∞–≤–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å >= thresh –Ω–µ–ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"
                X = X.dropna(axis=0, subset=subset, thresh=int(thresh))
            else:
                how = step.get("how", "any")
                X = X.dropna(axis=0, subset=subset, how=how)

            removed = before - len(X)
            if removed:
                typer.echo(f"   ‚Ä¢ dropna_rows: removed {removed} rows")

        elif op == "dropna_columns":
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤.
        # –†–µ–∂–∏–º—ã:
        #   A) —Ç–æ–ª—å–∫–æ cols -> —É–¥–∞–ª–∏—Ç—å –∏—Ö –±–µ–∑—É—Å–ª–æ–≤–Ω–æ
        #   B) —Ç–æ–ª—å–∫–æ min_missing_ratio -> —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å NaN-–¥–µ–ª—å—Ç–æ–π >= –ø–æ—Ä–æ–≥–∞
        #   C) cols + min_missing_ratio -> –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¢–û–õ–¨–ö–û cols –∏ —É–¥–∞–ª–∏—Ç—å —Ç–µ, –≥–¥–µ –¥–æ–ª—è NaN >= –ø–æ—Ä–æ–≥–∞
            cols = step.get("cols")
            if isinstance(cols, str):
                cols = [cols]
            min_ratio = step.get("min_missing_ratio", None)

            to_drop = set()
            if min_ratio is None:
                # A) –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–π –¥—Ä–æ–ø –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                if cols:
                    to_drop.update(cols)
            else:
                # B/C) –ø–æ—Ä–æ–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞
                if cols:
                    candidates = [c for c in cols if c in X.columns]
                else:
                    candidates = list(X.columns)
                if candidates:
                    ratios = X[candidates].isna().mean()
                    auto_drop = ratios[ratios >= float(min_ratio)].index.tolist()
                    to_drop.update(auto_drop)

            if to_drop:
                existing = [c for c in to_drop if c in X.columns]
                if existing:
                    X = X.drop(columns=existing, errors="ignore")
                    typer.echo(f"   ‚Ä¢ dropna_columns: dropped {len(existing)} columns: {existing[:10]}{'...' if len(existing)>10 else ''}")


        elif op == "drop_duplicates":
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤—Å–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã):
            #   subset: "all" | str | list[str] ‚Äî –ø–æ –∫–∞–∫–∏–º –∫–æ–ª–æ–Ω–∫–∞–º –∏—Å–∫–∞—Ç—å –¥—É–±–ª–∏ ("all" = –ø–æ –≤—Å–µ–º)
            #   keep: "first"|"last"|False      ‚Äî –∫–∞–∫–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –æ—Å—Ç–∞–≤–∏—Ç—å (–¥–µ—Ñ–æ–ª—Ç 'first'; False = —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –ø–æ–≤—Ç–æ—Ä—ã)
            #   ignore_index: bool              ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã (–¥–µ—Ñ–æ–ª—Ç True)
            subset = step.get("subset")
            if subset == "all":
                subset = None  # pandas: None => –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏
            elif isinstance(subset, str):
                subset = [subset]

            keep = step.get("keep", "first")
            ignore_index = bool(step.get("ignore_index", True))

            before = len(X)
            X = X.drop_duplicates(subset=subset, keep=keep, ignore_index=ignore_index)
            removed = before - len(X)
            if removed:
                typer.echo(f"   ‚Ä¢ drop_duplicates: removed {removed} rows")

        elif op == "rename_columns":
            # —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–π —à–∞–≥: {"old":"new", ...}
            mapping = step["mapping"]
            X = X.rename(columns=mapping)

        elif op == "drop_columns":
            cols = step["cols"]
            X = X.drop(columns=cols, errors="ignore")
            
        elif op == "select_columns":
            include = step.get("include")
            exclude = step.get("exclude")

            if include and exclude:
                raise typer.BadParameter("select_columns: use either 'include' or 'exclude', not both")

            if include:
                if isinstance(include, str):
                    include = [include]
                keep = [c for c in include if c in X.columns]
                missing = sorted(set(include) - set(keep))
                if missing:
                    typer.echo(f"   ‚Ä¢ select_columns: missing {missing[:10]}{'...' if len(missing)>10 else ''}")
                X = X[keep]
            elif exclude:
                if isinstance(exclude, str):
                    exclude = [exclude]
                X = X[[c for c in X.columns if c not in set(exclude)]]
            else:
                typer.echo("   ‚Ä¢ select_columns: nothing to do (no include/exclude)")

        elif op == "join":
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            #  right: str (–ø—É—Ç—å –∫ —Ñ–∞–π–ª—É .csv/.parquet)
            #  on: str|list[str]  (–∏–ª–∏ left_on/right_on)
            #  how: left|inner|outer (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é left)
            #  select: list[str]  ‚Äî –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∞–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç–∞–≤–∏—Ç—å (–ø–ª—é—Å –∫–ª—é—á–∏)
            #  suffix_right: str  ‚Äî —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∫–æ–ª–ª–∏–∑–∏–π –∏–º—ë–Ω
            right = step["right"]
            how = step.get("how", "left")
            on = step.get("on")
            left_on = step.get("left_on")
            right_on = step.get("right_on")
            select = step.get("select")
            suffix_right = step.get("suffix_right", "_r")

            rdf = _load_df({"input": right})
            if select:
                keys = on if isinstance(on, list) else ([on] if on else [])
                if left_on and right_on:
                    keys = [right_on] if isinstance(right_on, str) else list(right_on)
                keep = list(dict.fromkeys(list(keys) + list(select)))
                rdf = rdf[[c for c in keep if c in rdf.columns]]

            prev_cols = set(X.columns)
            if on:
                X = X.merge(rdf, how=how, on=on, suffixes=(None, suffix_right))
            else:
                X = X.merge(rdf, how=how, left_on=left_on, right_on=right_on, suffixes=(None, suffix_right))
            typer.echo(f"   ‚Ä¢ join {Path(right).name}: +{len([c for c in X.columns if c not in prev_cols])} cols")

        else:
            raise ValueError(f"Unknown op '{op}' in step #{i}")
            

    return X

# ======== –ö–æ–º–∞–Ω–¥—ã CLI ========

@app.command()
def apply(
    input: str = typer.Argument(..., help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.csv/.parquet)"),
    output: str = typer.Argument(..., help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"),
    steps_json: str = typer.Option(None, help="JSON —Å–æ —Å–ø–∏—Å–∫–æ–º —à–∞–≥–æ–≤"),
    sample: float = typer.Option(None, help="–î–æ–ª—è —Å—ç–º–ø–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –Ω–∞–ø—Ä. 0.1"),
):
    """–ü—Ä–∏–º–µ–Ω–∏—Ç—å —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ –æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (–±–µ–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞).

    Example:
        >>> preproc apply data/raw.csv data/interim/cli_related/clean.parquet \
        ...   --steps-json '[{"op":"lowercase_categoricals", "cat_cols":["customer_city"]}]'

    Args:
        input (str): –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.csv –∏–ª–∏ .parquet).
        output (str): –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        steps_json (str | None): JSON —Å–æ —Å–ø–∏—Å–∫–æ–º —à–∞–≥–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
        sample (float | None): –î–æ–ª—è —Å—ç–º–ø–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä ``0.1``.
    
    Returns:
        None
    """
    df = pd.read_parquet(input) if input.endswith(".parquet") else pd.read_csv(input)
    if sample:
        df = df.sample(frac=float(sample), random_state=42)
    steps = json.loads(steps_json) if steps_json else []
    df_out = _apply_steps(df, steps)
    _save_df(df_out, output)

@app.command()
def run(manifest: str = typer.Option("preprocessings/preprocessing_manifest.yaml", "--manifest", "-m", help="–ü—É—Ç—å –∫ YAML-–º–∞–Ω–∏—Ñ–µ—Å—Ç—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –Ω–∞–±–æ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ –º–∞–Ω–∏—Ñ–µ—Å—Ç—É YAML.
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø–æ –¥—É—Ö—É —Å validator_cli: defaults + datasets[].
    """
    cfg = yaml.safe_load(Path(manifest).read_text())
    defaults = cfg.get("defaults", {})

    errors_total: list[str] = []

    for ds in cfg["datasets"]:
        if ds.get("enabled") is False:
            continue

        name = ds["name"]
        typer.echo(f"‚ñ∂ {name}")

        # 1) –∑–∞–≥—Ä—É–∑–∫–∞
        try:
            df = _load_df(ds)   # —Ç–∞ –∂–µ –∏–¥–µ—è, —á—Ç–æ –∏ –≤ validator_cli
        except Exception as e:
            msg = f"load error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 2) sample (–∫–∞–∫ –≤ validator_cli.profile_all)
        sample = ds.get("sample", defaults.get("sample"))
        if sample:
            df = df.sample(frac=float(sample), random_state=42)

        # 3) —à–∞–≥–∏
        try:
            steps = ds.get("steps", [])
            df_out = _apply_steps(df, steps, defaults=defaults)
        except Exception as e:
            msg = f"processing error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 4) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        try:
            output = ds["output"]
            _save_df(df_out, output)
            typer.echo(f"‚úÖ {name}: OK")
        except Exception as e:
            msg = f"save error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")

    if errors_total:
        raise typer.Exit(code=1)

    typer.echo("‚úÖ –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
    
@app.command("label")
def make_label(
    input_path: Path = typer.Option(..., help="–ü—É—Ç—å –∫ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç—É –ø–æ—Å–ª–µ join-–æ–≤"),
    output_path: Path = typer.Option(..., help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å —Ç–∞—Ä–≥–µ—Ç–æ–º"),
    customer_col: str = "customer_id",
    purchase_ts_col: str = "order_purchase_timestamp",
    target_col: str = "churned",
    horizon_days: int = 120,
    reference_date: str = "max",  # "max" –∏–ª–∏ '2018-09-01'
    filter_status_col: str = "order_status",
    keep_statuses: str = "delivered",  # —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    force: bool = False,
):
    df = pd.read_parquet(input_path) if input_path.suffix==".parquet" else pd.read_csv(input_path)
    ks = tuple(s.strip() for s in keep_statuses.split(",")) if keep_statuses else None

    df_out = create_churn_label(
        df,
        customer_col=customer_col,
        purchase_ts_col=purchase_ts_col,
        target_col=target_col,
        horizon_days=horizon_days,
        reference_date=reference_date,
        filter_status_col=filter_status_col,
        keep_statuses=ks,
        force=force,
    )

    if output_path.suffix==".parquet":
        df_out.to_parquet(output_path, index=False)
    else:
        df_out.to_csv(output_path, index=False)
    typer.echo(f"Saved with target to: {output_path}")
    
cli = get_command(app)

if __name__ == "__main__":
    app()
