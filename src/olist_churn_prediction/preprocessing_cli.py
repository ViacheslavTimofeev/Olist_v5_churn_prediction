"""–ú–∞–Ω–∏—Ñ–µ—Å—Ç‚Äëdriven –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Olist Churn.

–ú–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç CLI-–∫–æ–º–∞–Ω–¥—ã (Typer) –∏ —è–¥—Ä–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä–æ–µ
–ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ –∫ –≤—Ö–æ–¥–Ω–æ–º—É ``DataFrame``.

–û—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:

- :func:`_load_df` ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (glob) –∏–ª–∏ SQL.
- :func:`_save_df` ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ ``.parquet``/``.csv``.
- :func:`_apply_steps` ‚Äî –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–≥–æ–≤ –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞.
- CLI-–∫–æ–º–∞–Ω–¥—ã: :func:`apply`, :func:`run`, :func:`make_label`.

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ (–∏–∑ –∫–æ–Ω—Å–æ–ª–∏)::

    # –µ–¥–∏–Ω–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞
    python -m olist_churn_prediction.preprocessing_cli apply data/raw/orders.parquet data/interim/orders_clean.parquet --steps-json '[{"op":"lowercase_categoricals"}]'

    # –ø–∞–∫–µ—Ç–Ω—ã–π —Ä–µ–∂–∏–º —Å –º–∞–Ω–∏—Ñ–µ—Å—Ç–æ–º
    python -m olist_churn_prediction.preprocessing_cli run --manifest preprocessings/preprocessing_manifest.yaml
"""

from __future__ import annotations

import json
import glob
import os
import yaml
import typer
from typer.main import get_command
from pathlib import Path
from typing import Dict, Any, Union, List, Callable
import pandas as pd
from olist_churn_prediction.targets import create_churn_label
from olist_churn_prediction import feature_processing as fp  # lowercase_categoricals, disambiguate_city_state, group_by_features

app = typer.Typer()


def _load_df(entry: Dict[str, Any]) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ``DataFrame`` –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –¥–≤–∞ —Ä–µ–∂–∏–º–∞:
    - ``"file"`` ‚Äî —á–∏—Ç–∞–µ—Ç **–ø–æ—Å–ª–µ–¥–Ω–∏–π** (–ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏) —Ñ–∞–π–ª –ø–æ
    glob-–º–∞—Å–∫–µ –∏–∑ ``entry["input"]``; —Ñ–æ—Ä–º–∞—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é.
    - ``"sql"`` ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ—Ç SQL-–∑–∞–ø—Ä–æ—Å ``entry["query"]`` —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è ``entry["conn_env"]``.
    
    Args:
        entry: –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –ö–ª—é—á–∏ (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞):
            * ``reader``: ``"file"`` | ``"sql"`` (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ``"file"``).
            * –¥–ª—è ``file``: ``input`` ‚Äî glob-–º–∞—Å–∫–∞ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º.
            * –¥–ª—è ``sql``: ``query`` (SQL-—Å—Ç—Ä–æ–∫–∞), ``conn_env`` (–∏–º—è env-–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            —Å–æ —Å—Ç—Ä–æ–∫–æ–π –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è), –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ``params``.
    
    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
    
    Raises:
        KeyError: –ï—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞.
        FileNotFoundError: –ï—Å–ª–∏ –ø–æ glob-–º–∞—Å–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª.
        ValueError: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ``reader``.
    """
    reader = entry.get("reader", "file")
    if reader == "sql":
        import sqlalchemy as sa
        url = os.environ[entry["conn_env"]]
        engine = sa.create_engine(url)
        return pd.read_sql(sa.text(entry["query"]), engine, params=entry.get("params"))
    else:
        path = sorted(glob.glob(entry["input"]))[-1]
        return pd.read_parquet(path) if path.endswith(".parquet") else pd.read_csv(path)


def _save_df(df: pd.DataFrame, output: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç ``DataFrame`` –Ω–∞ –¥–∏—Å–∫ –≤ ``.parquet`` –∏–ª–∏ ``.csv``.

    –ü–∞–ø–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    
    Args:
        df: –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
        output: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–æ–∫–æ–Ω—á–∞–Ω–∏–µ ``.parquet`` –∏–ª–∏ ``.csv``).
    
    Returns:
        None
    """
    out = Path(output)
    out.parent.mkdir(parents=True, exist_ok=True)
    if out.suffix == ".parquet":
        df.to_parquet(out, index=False)
    else:
        df.to_csv(out, index=False)
    typer.echo(f"üíæ Saved: {out}")

    
def _apply_steps(
    df: pd.DataFrame,
    steps: List[Dict[str, Any]],
    defaults: Dict[str, Any] | None = None,
) -> pd.DataFrame:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ ``DataFrame``.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (``op``):
    ``lowercase_categoricals``, ``disambiguate_city_state``,
    ``group_by_features``, ``groupby_aggregate``, ``dropna_rows``,
    ``dropna_columns``, ``drop_duplicates``, ``rename_columns``,
    ``drop_columns``, ``select_columns``, ``join``.
    
    Args:
        df: –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
        steps: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî —Å–ª–æ–≤–∞—Ä—å
            —Å –∫–ª—é—á–æ–º ``op`` –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏.
        defaults: –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã).
    
    Returns:
        –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.
    
    Raises:
        ValueError: –ï—Å–ª–∏ —à–∞–≥ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á ``op`` –∏–ª–∏ —É–∫–∞–∑–∞–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π ``op``.
    """
    defaults = defaults or {}
    X = df

    for i, step in enumerate(steps, start=1):
        op = step.get("op")
        if not op:
            raise ValueError(f"Step #{i} has no 'op'")

        if op == "lowercase_categoricals":
            cat_cols = step.get("cat_cols")
            if cat_cols is None:
                # –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ object/string/category
                cat_cols = [c for c in X.columns if str(X[c].dtype) in ("object", "string") or pd.api.types.is_categorical_dtype(X[c])]
            X = fp.lowercase_categoricals(X, cat_cols=cat_cols, inplace=False)

        elif op == "disambiguate_city_state":
            city_col = step["city_col"]
            state_col = step["state_col"]
            suffix_sep = step.get("suffix_sep", "_")
            X = fp.disambiguate_city_state(X, city_col, state_col, suffix_sep=suffix_sep, inplace=False)

        elif op == "group_by_features":
            group_mapping = step["group_mapping"]             # {"new_feat": ["col1","col2"], ...}
            agg_funcs     = step.get("agg_funcs", "sum")      # –º–æ–∂–µ—Ç –±—ã—Ç—å str|callable|list|dict
            keep_original = step.get("keep_original", False)
            prefix        = step.get("prefix")
            X = fp.group_by_features(
                X, group_mapping=group_mapping, agg_funcs=agg_funcs,
                keep_original=keep_original, prefix=prefix
            )

        elif op == "groupby_aggregate":
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É(–∞–º) —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–≥–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞–º–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º.
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            #   by: str | list[str]        ‚Äî –∫–ª—é—á(–∏) –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            #   sum_cols:  list[str]
            #   mean_cols: list[str]
            #   min_cols:  list[str]
            #   first_for_rest: bool=True  ‚Äî –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –±–µ—Ä—ë–º 'first'
            by = step["by"]
            if isinstance(by, str):
                by = [by]

            sum_cols  = step.get("sum_cols", []) or []
            mean_cols = step.get("mean_cols", []) or []
            min_cols  = step.get("min_cols", []) or []
            first_for_rest = bool(step.get("first_for_rest", True))

            # 1) —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –∞–≥–≥—Ä–µ–≥–∞—Ü–∏–π
            agg_dict = {}
            for c in sum_cols:  agg_dict[c]  = "sum"
            for c in mean_cols: agg_dict[c] = "mean"
            for c in min_cols:  agg_dict[c]  = "min"

            # 2) –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Äî 'first' (–∫—Ä–æ–º–µ –∫–ª—é—á–µ–π –∏ —É–∂–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö)
            if first_for_rest:
                selected = set(by) | set(sum_cols) | set(mean_cols) | set(min_cols)
                for c in X.columns:
                    if c not in selected:
                        agg_dict[c] = "first"

            # 3) —Å–∞–º groupby
            # dropna=False —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –≥—Ä—É–ø–ø—ã —Å NaN-–∫–ª—é—á–æ–º.
            X = (
                X.groupby(by, dropna=False)
                 .agg(agg_dict)
                 .reset_index()
            )
            
        elif op == "dropna_rows":
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏.
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        #   subset: str | list[str] ‚Äî –ø–æ –∫–∞–∫–∏–º –∫–æ–ª–æ–Ω–∫–∞–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å NaN (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî –ø–æ –≤—Å–µ–º)
        #   how: "any"|"all"        ‚Äî —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –µ—Å—Ç—å –ª—é–±–æ–π NaN ("any") –∏–ª–∏ –≤—Å–µ NaN ("all")
        #   thresh: int             ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ù–ï-–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –æ—Å—Ç–∞–ª–∞—Å—å (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ, 'how' –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
            subset = step.get("subset")
            if isinstance(subset, str):
                subset = [subset]

            thresh = step.get("thresh", None)
            before = len(X)

            if thresh is not None:
                # –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ –ø—Ä–∞–≤–∏–ª—É "–æ—Å—Ç–∞–≤–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å >= thresh –Ω–µ–ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"
                X = X.dropna(axis=0, subset=subset, thresh=int(thresh))
            else:
                how = step.get("how", "any")
                X = X.dropna(axis=0, subset=subset, how=how)

            removed = before - len(X)
            if removed:
                typer.echo(f"   ‚Ä¢ dropna_rows: removed {removed} rows")

        elif op == "dropna_columns":
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤.
        # –†–µ–∂–∏–º—ã:
        #   A) —Ç–æ–ª—å–∫–æ cols -> —É–¥–∞–ª–∏—Ç—å –∏—Ö –±–µ–∑—É—Å–ª–æ–≤–Ω–æ
        #   B) —Ç–æ–ª—å–∫–æ min_missing_ratio -> —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å NaN-–¥–µ–ª—å—Ç–æ–π >= –ø–æ—Ä–æ–≥–∞
        #   C) cols + min_missing_ratio -> –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¢–û–õ–¨–ö–û cols –∏ —É–¥–∞–ª–∏—Ç—å —Ç–µ, –≥–¥–µ –¥–æ–ª—è NaN >= –ø–æ—Ä–æ–≥–∞
            cols = step.get("cols")
            if isinstance(cols, str):
                cols = [cols]
            min_ratio = step.get("min_missing_ratio", None)

            to_drop = set()
            if min_ratio is None:
                # A) –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–π –¥—Ä–æ–ø –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                if cols:
                    to_drop.update(cols)
            else:
                # B/C) –ø–æ—Ä–æ–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞
                if cols:
                    candidates = [c for c in cols if c in X.columns]
                else:
                    candidates = list(X.columns)
                if candidates:
                    ratios = X[candidates].isna().mean()
                    auto_drop = ratios[ratios >= float(min_ratio)].index.tolist()
                    to_drop.update(auto_drop)

            if to_drop:
                existing = [c for c in to_drop if c in X.columns]
                if existing:
                    X = X.drop(columns=existing, errors="ignore")
                    typer.echo(f"   ‚Ä¢ dropna_columns: dropped {len(existing)} columns: {existing[:10]}{'...' if len(existing)>10 else ''}")


        elif op == "drop_duplicates":
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤—Å–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã):
            #   subset: "all" | str | list[str] ‚Äî –ø–æ –∫–∞–∫–∏–º –∫–æ–ª–æ–Ω–∫–∞–º –∏—Å–∫–∞—Ç—å –¥—É–±–ª–∏ ("all" = –ø–æ –≤—Å–µ–º)
            #   keep: "first"|"last"|False      ‚Äî –∫–∞–∫–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –æ—Å—Ç–∞–≤–∏—Ç—å (–¥–µ—Ñ–æ–ª—Ç 'first'; False = —É–¥–∞–ª–∏—Ç—å –≤—Å–µ –ø–æ–≤—Ç–æ—Ä—ã)
            #   ignore_index: bool              ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã (–¥–µ—Ñ–æ–ª—Ç True)
            subset = step.get("subset")
            if subset == "all":
                subset = None  # pandas: None => –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏
            elif isinstance(subset, str):
                subset = [subset]

            keep = step.get("keep", "first")
            ignore_index = bool(step.get("ignore_index", True))

            before = len(X)
            X = X.drop_duplicates(subset=subset, keep=keep, ignore_index=ignore_index)
            removed = before - len(X)
            if removed:
                typer.echo(f"   ‚Ä¢ drop_duplicates: removed {removed} rows")

        elif op == "rename_columns":
            # —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–π —à–∞–≥: {"old":"new", ...}
            mapping = step["mapping"]
            X = X.rename(columns=mapping)

        elif op == "drop_columns":
            cols = step["cols"]
            X = X.drop(columns=cols, errors="ignore")
            
        elif op == "select_columns":
            include = step.get("include")
            exclude = step.get("exclude")

            if include and exclude:
                raise typer.BadParameter("select_columns: use either 'include' or 'exclude', not both")

            if include:
                if isinstance(include, str):
                    include = [include]
                keep = [c for c in include if c in X.columns]
                missing = sorted(set(include) - set(keep))
                if missing:
                    typer.echo(f"   ‚Ä¢ select_columns: missing {missing[:10]}{'...' if len(missing)>10 else ''}")
                X = X[keep]
            elif exclude:
                if isinstance(exclude, str):
                    exclude = [exclude]
                X = X[[c for c in X.columns if c not in set(exclude)]]
            else:
                typer.echo("   ‚Ä¢ select_columns: nothing to do (no include/exclude)")

        elif op == "join":
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            #  right: str (–ø—É—Ç—å –∫ —Ñ–∞–π–ª—É .csv/.parquet)
            #  on: str|list[str]  (–∏–ª–∏ left_on/right_on)
            #  how: left|inner|outer (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é left)
            #  select: list[str]  ‚Äî –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∞–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç–∞–≤–∏—Ç—å (–ø–ª—é—Å –∫–ª—é—á–∏)
            #  suffix_right: str  ‚Äî —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∫–æ–ª–ª–∏–∑–∏–π –∏–º—ë–Ω
            right = step["right"]
            how = step.get("how", "left")
            on = step.get("on")
            left_on = step.get("left_on")
            right_on = step.get("right_on")
            select = step.get("select")
            suffix_right = step.get("suffix_right", "_r")

            rdf = _load_df({"input": right})
            if select:
                keys = on if isinstance(on, list) else ([on] if on else [])
                if left_on and right_on:
                    keys = [right_on] if isinstance(right_on, str) else list(right_on)
                keep = list(dict.fromkeys(list(keys) + list(select)))
                rdf = rdf[[c for c in keep if c in rdf.columns]]

            prev_cols = set(X.columns)
            if on:
                X = X.merge(rdf, how=how, on=on, suffixes=(None, suffix_right))
            else:
                X = X.merge(rdf, how=how, left_on=left_on, right_on=right_on, suffixes=(None, suffix_right))
            typer.echo(f"   ‚Ä¢ join {Path(right).name}: +{len([c for c in X.columns if c not in prev_cols])} cols")

        else:
            raise ValueError(f"Unknown op '{op}' in step #{i}")
            

    return X


@app.command()
def apply(
    input: str = typer.Argument(..., help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (.csv/.parquet)"),
    output: str = typer.Argument(..., help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"),
    steps_json: str = typer.Option(None, help="JSON —Å–æ —Å–ø–∏—Å–∫–æ–º —à–∞–≥–æ–≤"),
    sample: float = typer.Option(None, help="–î–æ–ª—è —Å—ç–º–ø–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –Ω–∞–ø—Ä. 0.1"),
) -> None:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ –æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (–±–µ–∑ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞).

    –ü—Ä–∏–º–µ—Ä:

    preproc apply data/raw.csv data/interim/clean.parquet \
      --steps-json '[{"op":"lowercase_categoricals", "cat_cols":["customer_city"]}]'
    
    Args:
        input: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (``.csv`` –∏–ª–∏ ``.parquet``).
        output: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        steps_json: JSON-—Å—Ç—Ä–æ–∫–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º —à–∞–≥–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
        sample: –î–æ–ª—è —Å—ç–º–ø–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ``0.1``).
    
    Returns:
        None
    """
    df = pd.read_parquet(input) if input.endswith(".parquet") else pd.read_csv(input)
    if sample:
        df = df.sample(frac=float(sample), random_state=42)
    steps = json.loads(steps_json) if steps_json else []
    df_out = _apply_steps(df, steps)
    _save_df(df_out, output)


@app.command()
def run(manifest: str = typer.Option(
    "preprocessings/preprocessing_manifest.yaml",
    "--manifest", "-m",
    help="–ü—É—Ç—å –∫ YAML-–º–∞–Ω–∏—Ñ–µ—Å—Ç—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )
) -> None:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞–∫–µ—Ç–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ YAML-–º–∞–Ω–∏—Ñ–µ—Å—Ç—É.

    –ß–∏—Ç–∞–µ—Ç —Å–µ–∫—Ü–∏–∏ ``defaults`` –∏ ``datasets``. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
      1) –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ (:func:`_load_df`),
      2) –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—ç–º–ø–ª–∏—Ä—É–µ—Ç,
      3) –ø—Ä–∏–º–µ–Ω—è–µ—Ç :func:`_apply_steps`,
      4) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    
    Args:
        manifest: –ü—É—Ç—å –∫ YAML-–º–∞–Ω–∏—Ñ–µ—Å—Ç—É –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
    
    Raises:
        typer.Exit: –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏,
            –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    
    Returns:
        None
    """
    cfg = yaml.safe_load(Path(manifest).read_text())
    defaults = cfg.get("defaults", {})

    errors_total: list[str] = []

    for ds in cfg["datasets"]:
        if ds.get("enabled") is False:
            continue

        name = ds["name"]
        typer.echo(f"‚ñ∂ {name}")

        # 1) –∑–∞–≥—Ä—É–∑–∫–∞
        try:
            df = _load_df(ds)   # —Ç–∞ –∂–µ –∏–¥–µ—è, —á—Ç–æ –∏ –≤ validator_cli
        except Exception as e:
            msg = f"load error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 2) sample (–∫–∞–∫ –≤ validator_cli.profile_all)
        sample = ds.get("sample", defaults.get("sample"))
        if sample:
            df = df.sample(frac=float(sample), random_state=42)

        # 3) —à–∞–≥–∏
        try:
            steps = ds.get("steps", [])
            df_out = _apply_steps(df, steps, defaults=defaults)
        except Exception as e:
            msg = f"processing error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")
            continue

        # 4) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        try:
            output = ds["output"]
            _save_df(df_out, output)
            typer.echo(f"‚úÖ {name}: OK")
        except Exception as e:
            msg = f"save error: {e}"
            typer.echo(f"‚ùå {name}: {msg}")
            errors_total.append(f"{name}: {msg}")

    if errors_total:
        raise typer.Exit(code=1)

    typer.echo("‚úÖ –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")


@app.command("label")
def make_label(
    input_path: Path = typer.Option(..., help="–ü—É—Ç—å –∫ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç—É –ø–æ—Å–ª–µ join-–æ–≤"),
    output_path: Path = typer.Option(..., help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å —Ç–∞—Ä–≥–µ—Ç–æ–º"),
    customer_col: str = "customer_id",
    purchase_ts_col: str = "order_purchase_timestamp",
    target_col: str = "churned",
    horizon_days: int = 120,
    reference_date: str = "max",  # "max" –∏–ª–∏ '2018-09-01'
    filter_status_col: str = "order_status",
    keep_statuses: str = "delivered",  # —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    force: bool = False,
) -> None:
    """–°–æ–∑–¥–∞—ë—Ç —Å—Ç–æ–ª–±–µ—Ü —Ç–∞—Ä–≥–µ—Ç–∞ –æ—Ç—Ç–æ–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.
    
    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç :func:`olist_churn_prediction.targets.create_churn_label`.
    
    Args:
        input_path: –ü—É—Ç—å –∫ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç—É (``.csv``/``.parquet``) –ø–æ—Å–ª–µ join-–æ–≤.
        output_path: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ç–∞—Ä–≥–µ—Ç–æ–º.
        customer_col: –ò–º—è —Å—Ç–æ–ª–±—Ü–∞ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –∫–ª–∏–µ–Ω—Ç–∞.
        purchase_ts_col: –ò–º—è —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–æ–π/–≤—Ä–µ–º–µ–Ω–µ–º –ø–æ–∫—É–ø–∫–∏.
        target_col: –ò–º—è —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞-—Ç–∞—Ä–≥–µ—Ç–∞.
        horizon_days: –ì–æ—Ä–∏–∑–æ–Ω—Ç –¥–∞–≤–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç—Ç–æ–∫–∞ (–≤ –¥–Ω—è—Ö).
        reference_date: –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –¥–∞—Ç–∞ (``"max"`` –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ ``YYYY-MM-DD``).
        filter_status_col: –ò–º—è —Å—Ç–æ–ª–±—Ü–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–∫–∞–∑–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
        keep_statuses: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç—É—Å–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é), –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç–∞–≤–∏—Ç—å.
        force: –ü–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–∞—Ä–≥–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å—Ç–æ–ª–±—Ü–∞ ``target_col``.
    
    Returns:
        None
    """
    df = pd.read_parquet(input_path) if input_path.suffix==".parquet" else pd.read_csv(input_path)
    ks = tuple(s.strip() for s in keep_statuses.split(",")) if keep_statuses else None

    df_out = create_churn_label(
        df,
        customer_col=customer_col,
        purchase_ts_col=purchase_ts_col,
        target_col=target_col,
        horizon_days=horizon_days,
        reference_date=reference_date,
        filter_status_col=filter_status_col,
        keep_statuses=ks,
        force=force,
    )

    if output_path.suffix==".parquet":
        df_out.to_parquet(output_path, index=False)
    else:
        df_out.to_csv(output_path, index=False)
    typer.echo(f"Saved with target to: {output_path}")
    
cli = get_command(app)

if __name__ == "__main__":
    app()
